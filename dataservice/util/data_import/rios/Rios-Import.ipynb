{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "DATA_DIR = '/Users/singhn4/Projects/kids_first/data/Rios_Wise_2016'\n",
    "DBGAP_DIR = os.path.join(DATA_DIR, 'dbgap')\n",
    "MANIFESTS_DIR = os.path.join(DATA_DIR, 'manifests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def read_json(filepath):\n",
    "    with open(filepath, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def write_json(data, filepath):\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        json.dump(data, json_file, sort_keys=True, indent=4, separators=(',', ':'))\n",
    "        \n",
    "def cols_to_lower(df):\n",
    "    df.columns = map((lambda x: x.replace(\" \", \"_\").lower()), df.columns)\n",
    "        \n",
    "def dropna_rows_cols(df_func):\n",
    "    \"\"\"\n",
    "    Decorator to drop rows and cols w all nan values\n",
    "    Replace NaN values with None\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        df = df_func(*args, **kwargs)\n",
    "\n",
    "        # None or empty df\n",
    "        try:\n",
    "            if df.empty:\n",
    "                return df\n",
    "        except AttributeError:\n",
    "            return df\n",
    "\n",
    "        # Rows\n",
    "        df.dropna(how=\"all\", inplace=True)\n",
    "        # Cols\n",
    "        df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "        # Replace NaN values with None\n",
    "        df = df.where((pd.notnull(df)), None)\n",
    "        return df\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def reformat_column_names(df_func):\n",
    "    \"\"\"\n",
    "    Decorator to reformat DataFrame column names.\n",
    "\n",
    "    Replace all column names having whitespace with underscore\n",
    "    and make lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        df = df_func(*args, **kwargs)\n",
    "        # None or empty df\n",
    "        try:\n",
    "            if df.empty:\n",
    "                return df\n",
    "        except AttributeError:\n",
    "            return df\n",
    "        \n",
    "        cols_to_lower(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study\n",
    "study = {\n",
    "    'data_access_authority': 'dbGaP',\n",
    "    'study_id': 'phs001410',\n",
    "    'study_version': 'v1.p2',\n",
    "    'study_name': 'Genomics of Orthopaedic Disease Program',\n",
    "    'attribution': None\n",
    "}\n",
    "study_df = pd.DataFrame([study])\n",
    "study_df.to_csv(os.path.join(DATA_DIR, 'study.txt'), index=False)\n",
    "\n",
    "# Create investigator\n",
    "invest = {\n",
    "    'investigator_name': 'Jonathan Rios',\n",
    "    'institution': 'UT Southwestern Medical Center'\n",
    "}\n",
    "inv_df = pd.DataFrame([invest])\n",
    "inv_df.to_csv(os.path.join(DATA_DIR, 'investigator.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_study_file_data(filepaths=None):\n",
    "    \"\"\"\n",
    "    Read in raw study files\n",
    "    \"\"\"\n",
    "    if not filepaths:\n",
    "        filepaths = os.listdir(DBGAP_DIR)\n",
    "\n",
    "    study_files = [{\"study_file_name\": f}\n",
    "                   for f in filepaths if 'dbGaP' in f]\n",
    "    return pd.DataFrame(study_files)\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_study_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read study data\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR,\n",
    "                                'study.txt')\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_investigator_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read investigator data\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR,\n",
    "                                'investigator.txt')\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_subject_data(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR, 'HL13237501A1_V3_SubjectDS.txt')\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', dtype={'SUBJID': str})\n",
    "    df = df[['SUBJECT_ID', 'CONSENT']]\n",
    "    \n",
    "    # Decode consent ints to consent strings\n",
    "    def func(row): \n",
    "        _map = {0:None, \n",
    "                1: \"Health/Medical/Biomedical (IRB)\", \n",
    "                2: \"Disease-Specific (Musculoskeletal Diseases, IRB)(DS-MUS-SKEL-IRB)\"}\n",
    "        return _map[row['CONSENT']]\n",
    "    df['CONSENT'] = df.apply(func, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_phenotype_data(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SubjectPhenotypesDS.txt')\n",
    "    df = pd.read_csv(filepath, \n",
    "                    delimiter='\\t', \n",
    "                    dtype={'SUBJID': str})\n",
    "    \n",
    "    # Decode sex ints to gender strings\n",
    "    def func(row): \n",
    "        _map = {1: \"male\", 2: \"female\"}\n",
    "        return _map[row['Sex']]\n",
    "    df['Sex'] = df.apply(func, axis=1)\n",
    "\n",
    "    # Decode affected status ints to strings\n",
    "    def func(row): \n",
    "        _map = {0:'unknown', 1: \"not affected\", 2: \"affected\"}\n",
    "        return _map[row['AFFSTAT']]\n",
    "    df['AFFSTAT'] = df.apply(func, axis=1)\n",
    "\n",
    "    # Decode proband ints to booleans\n",
    "    def func(row): \n",
    "        _map = {1: True, 2: False}\n",
    "        return _map[row['Proband']]\n",
    "    df['Proband'] = df.apply(func, axis=1)\n",
    "    \n",
    "    # Create ethnicity column\n",
    "    _map = {'Hispanic': 'hispanic or latino'}\n",
    "    df['ethnicity'] = df['Race'].apply(lambda x: _map.get(x, 'not hispanic or latino'))\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_family_data(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_PedgreeDS.txt')\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', dtype={'SUBJID': str})\n",
    "    del df['SEX']\n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def create_participant_data():\n",
    "    \"\"\"\n",
    "    Create participant data from \n",
    "    \"\"\"\n",
    "    # Subject file\n",
    "    subject_df = read_subject_data()\n",
    "    # Phenotype file\n",
    "    phenotypes_df = read_phenotype_data()\n",
    "    # Family file\n",
    "    family_df = read_family_data()\n",
    "    \n",
    "    # Merge subject + phenotype\n",
    "    df1 = pd.merge(subject_df, phenotypes_df, on='subject_id')\n",
    "    \n",
    "    # Merge family\n",
    "    df = pd.merge(df1, family_df, on='subject_id')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_diagnosis_df(phenotype_df):\n",
    "    \"\"\"\n",
    "    Create diagnosis df from phenotype df\n",
    "    \"\"\"\n",
    "    def func(row): \n",
    "        _map = {'affected':'adolescent idiopathic scoliosis', \n",
    "                'not affected': None}\n",
    "        return _map.get(row['affstat'], row['affstat'])\n",
    "    phenotype_df['diagnosis'] = phenotype_df.apply(func, axis=1)\n",
    "    \n",
    "    return phenotype_df[['subject_id', 'diagnosis']]\n",
    "\n",
    "def create_phenotype_df(phenotype_df):\n",
    "    \"\"\"\n",
    "    Create phenotype df from original phenotype_df\n",
    "    \"\"\"\n",
    "    # Extract columns\n",
    "    phenotype_df = phenotype_df[['subject_id', 'affstat']]\n",
    "    # Drop unknowns\n",
    "    phenotype_df = phenotype_df[phenotype_df.affstat != 'unknown']\n",
    "    \n",
    "    # Add columns\n",
    "    def func(row): \n",
    "        _map = {'affected':'positive', \n",
    "                'not affected': 'negative'}\n",
    "        return _map.get(row['affstat'], row['affstat'])\n",
    "    \n",
    "    phenotype_df['observed'] = phenotype_df.apply(func, axis=1)\n",
    "    phenotype_df['hpo_id'] = 'HP:0002650'\n",
    "    phenotype_df['phenotype'] = 'adolescent idiopathic scoliosis'\n",
    "    return phenotype_df\n",
    "df = create_diagnosis_df(read_phenotype_data())\n",
    "df.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_sample_attr_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read sample attributes file\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SampleAttributesDS.txt')\n",
    "    return pd.read_csv(filepath, delimiter='\\t')\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_subject_sample_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read subject sample mapping file\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SubjectSampleMappingDS.txt')\n",
    "    return pd.read_csv(filepath, delimiter='\\t')\n",
    "\n",
    "# Sample attributes file\n",
    "sample_attr_df = read_sample_attr_data()\n",
    "sample_attr_df.shape\n",
    "# Subject sample file\n",
    "subject_sample_df = read_subject_sample_data()\n",
    "# Subject file\n",
    "subject_df = read_subject_data()\n",
    "\n",
    "# Merge sample attributes w subject sample\n",
    "df1 = pd.merge(sample_attr_df, subject_sample_df, on='sample_id')\n",
    "# Merge sample with subject\n",
    "sample_df = pd.merge(df1, subject_df, on='subject_id')\n",
    "sample_df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_seq_exp_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read sequencing experiment data\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(MANIFESTS_DIR, 'manifest_171210.csv')\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Sample Description'] = df['Sample Description'].apply(lambda x: x.split(':')[-1].strip())\n",
    "    df.describe(include=['O']).T.sort_values('unique', ascending=False)\n",
    "    \n",
    "    # Subject sample mapping\n",
    "    filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SubjectSampleMappingDS.txt')\n",
    "    subject_sample_df = pd.read_csv(filepath, delimiter='\\t')\n",
    "\n",
    "    # Merge with subject samples\n",
    "    df = pd.merge(subject_sample_df, df, left_on='SAMPLE_ID', right_on='Sample Description')\n",
    "\n",
    "    # Add unique col\n",
    "    def func(row): return \"_\".join(['seq_exp', str(row.name)])\n",
    "    df['seq_exp_id'] = df.apply(func, axis=1)\n",
    "\n",
    "    return df\n",
    "df = read_seq_exp_data()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Db gap files\n",
    "files = {f:os.path.join(DBGAP_DIR, f) for f in os.listdir(DBGAP_DIR)}\n",
    "pprint(list(files.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(files['HL13237501A1_V3_SubjectDS.txt'], delimiter='\\t', dtype={'SUBJID': str})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe(include=['O']).T.sort_values('unique', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject \n",
    "df2 = pd.read_csv(files['HL132375-01A1_V2_SubjectDS.txt'], delimiter='\\t', dtype={'SUBJID': str})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include=['O']).T.sort_values('unique', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2, on='SUBJECT_ID')\n",
    "df3.describe(include=['O']).T.sort_values('unique', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family/Pedigree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(files['HL132375-01A1_V2_PedgreeDS.txt'], delimiter='\\t', dtype={'SUBJID': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['O']).T.sort_values('unique', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(files['HL132375-01A1_V2_SubjectPhenotypesDS.txt'], delimiter='\\t', dtype={'SUBJID': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample attributes\n",
    "filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SampleAttributesDS.txt')\n",
    "df = pd.read_csv(filepath, delimiter='\\t')\n",
    "cols_to_lower(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.histological_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject sample mapping\n",
    "filepath = os.path.join(DBGAP_DIR, 'HL132375-01A1_V2_SubjectSampleMappingDS.txt')\n",
    "df = pd.read_csv(filepath, delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant + Demographic df\n",
    "participant_df = create_participant_data()\n",
    "participant_df.head()\n",
    "participant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Families and Proband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of families {}'.format(participant_df['family_id'].nunique()))\n",
    "print('# of probands {}'.format(participant_df[participant_df['proband'] == True]['proband'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Families without a Proband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = participant_df.groupby(['family_id'])[['subject_id', 'affstat', 'proband', 'family_id']]\n",
    "p = g.describe()['proband']\n",
    "p[p['unique'] != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participants Affected but NOT a Proband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = participant_df[(participant_df['affstat'] == 'affected') & (participant_df['proband'] == False)]['subject_id'].nunique()\n",
    "print('# of affected participants that are not probands {} '.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family df\n",
    "family_df = read_family_data()\n",
    "mothers = pd.merge(family_df[['subject_id', 'family_id']], family_df[['mother', 'father']], left_on='subject_id', right_on='mother')\n",
    "mothers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fathers = pd.merge(family_df[['subject_id', 'family_id']], family_df[['mother', 'father']], left_on='subject_id', right_on='father')\n",
    "fathers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenotype df\n",
    "phenotype_df = read_phenotype_data()\n",
    "phenotype_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis\n",
    "diagnosis_df = create_diagnosis_df(phenotype_df)\n",
    "diagnosis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genomic files\n",
    "\n",
    "# Seq Exp Data\n",
    "seq_exp_df = read_seq_exp_data()\n",
    "seq_exp_df = seq_exp_df[['subject_id', 'library', 'sample_id', 'sample_description', 'seq_exp_id']]\n",
    "seq_exp_df\n",
    "\n",
    "# Genomic file info\n",
    "uuid_dict= read_json(os.path.join(DATA_DIR,'genomic_files_by_uuid.json'))\n",
    "gf_df = pd.DataFrame(list(uuid_dict.values()))\n",
    "# gf_df['library'] = gf_df['file_name'].apply(lambda fn: fn.split('.')[0])\n",
    "gf_df['library'] = gf_df['urls'].apply(lambda urls: os.path.dirname(urls[0]).split('/')[-1])\n",
    "\n",
    "# Merge\n",
    "df = pd.merge(seq_exp_df, gf_df, on='library')\n",
    "\n",
    "# Reformat\n",
    "df['md5sum'] = df['hashes'].apply(lambda x: x['md5'])\n",
    "df['file_url'] = df['urls'].apply(lambda x: x[0])\n",
    "df['file_format'] = df['file_name'].apply(lambda x: '.'.join(x.split('.')[1:]))\n",
    "df.rename(columns={'did':'uuid', 'size':'file_size'}, inplace=True)\n",
    "\n",
    "def func(x):\n",
    "        x = x.strip()\n",
    "        if x == 'bam':\n",
    "            val = 'submitted aligned reads'\n",
    "        elif ('fastq' in x):\n",
    "            val = 'submitted reads'\n",
    "        elif 'vcf' in x:\n",
    "            val = 'variant calling'\n",
    "        else:\n",
    "            val = None\n",
    "        return val\n",
    "df['data_type'] = df['file_format'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
