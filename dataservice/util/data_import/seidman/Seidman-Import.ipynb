{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dataservice.util.data_import.utils import (\n",
    "    dropna_rows_cols,\n",
    "    reformat_column_names,\n",
    "    read_json, \n",
    "    write_json\n",
    ")\n",
    "\n",
    "DATA_DIR = '/Users/singhn4/Projects/kids_first/data/Seidman_2015'\n",
    "DBGAP_DIR = os.path.join(DATA_DIR, 'dbgap')\n",
    "ALIQUOT_SHIP_DIR = os.path.join(DATA_DIR, 'manifests', 'shipping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction - Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_study_file_data(filepaths=None):\n",
    "    \"\"\"\n",
    "    Read in raw study files\n",
    "    \"\"\"\n",
    "    if not filepaths:\n",
    "        filepaths = os.listdir(DBGAP_DIR)\n",
    "\n",
    "    study_files = [{\"study_file_name\": f}\n",
    "                   for f in filepaths if 'dbGaP' in f]\n",
    "    return pd.DataFrame(study_files)\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_study_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read study data\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR,\n",
    "                                'study.txt')\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_investigator_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read investigator data\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR,\n",
    "                                'investigator.txt')\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    return df\n",
    "\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_family_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read family data for all participants\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR,\n",
    "                                '7a_dbGaP_PedigreeDS.txt')\n",
    "    df = pd.read_csv(filepath,\n",
    "                     delimiter='\\t',\n",
    "                     dtype={'SUBJID': str})\n",
    "    # Subset of columns\n",
    "    df.drop(['SEX'], axis=1, inplace=True)\n",
    "\n",
    "    # Add proband column\n",
    "    def func(row): return bool(row['MOTHER'] and row['FATHER'])\n",
    "    df['is_proband'] = df.apply(func, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_phenotype_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read phenotype data\n",
    "    \"\"\"\n",
    "    # Read in cached phenotypes or create if they don't exist\n",
    "    hpo_fp = os.path.join(DATA_DIR, 'phenotype_hpo_mapping.txt')\n",
    "    if os.path.exists(hpo_fp):\n",
    "        return pd.read_csv(hpo_fp,dtype={'SUBJID': str})\n",
    "        \n",
    "    filepath = os.path.join(\n",
    "    DBGAP_DIR,\n",
    "    '3a_dbGaP_SubjectPhenotypes_ExtracardiacFindingsDS.txt')\n",
    "\n",
    "    # Read csv\n",
    "    df = pd.read_csv(filepath,\n",
    "                     delimiter='\\t',\n",
    "                     dtype={'SUBJID': str})\n",
    "\n",
    "    # Convert age years to days\n",
    "    df['LATEST_EXAM_AGE'] = df[\"LATEST_EXAM_AGE\"].apply(\n",
    "        lambda x: int(x) * 365)\n",
    "    age_at_event_days = df[['LATEST_EXAM_AGE', 'SUBJID']]\n",
    "\n",
    "    # Select string based phenotypes\n",
    "    df = df.select_dtypes(include='object')\n",
    "\n",
    "    # Make all values lower case\n",
    "    for col in df.columns.tolist():\n",
    "        df[col] = df[col].apply(lambda x: str(x).lower())\n",
    "\n",
    "    # Reshape to build the phenotypes df\n",
    "    cols = df.columns.tolist()[2:]\n",
    "    phenotype_cols = [col for col in cols if not col.startswith('OTHER')]\n",
    "    phenotype_df = pd.melt(df, id_vars='SUBJID', value_vars=phenotype_cols,\n",
    "                           var_name='phenotype', value_name='orig_observed')\n",
    "\n",
    "    # Remove unkonwns\n",
    "    unknown_values = ['none', 'unknown', 'no/not checked', 'not applicable', 'absent']\n",
    "    phenotype_df = phenotype_df[phenotype_df['orig_observed'].apply(lambda x: x not in unknown_values)]\n",
    "\n",
    "    # Add HPOs\n",
    "    from dataservice.util.data_import.etl.hpo import mapper\n",
    "    hpo_mapper = mapper.HPOMapper(DATA_DIR)\n",
    "    phenotype_df = hpo_mapper.add_hpo_id_col(phenotype_df)\n",
    "\n",
    "    # Map to positive/negative\n",
    "    def func(row): \n",
    "        return 'negative' if row['orig_observed'] == 'no' else 'positive'\n",
    "    phenotype_df['observed'] = phenotype_df.apply(func, axis=1)\n",
    "\n",
    "    # Merge back in age at event in days\n",
    "    phenotype_df = pd.merge(phenotype_df, age_at_event_days, on='SUBJID')\n",
    "\n",
    "    # Add unique col\n",
    "    def func(row): return \"_\".join(['phenotype', str(row.name)])\n",
    "    phenotype_df['phenotype_id'] = phenotype_df.apply(func, axis=1)\n",
    "    \n",
    "    # Write to file\n",
    "    phenotype_df.to_csv(hpo_fp, index=False)\n",
    "    \n",
    "    return phenotype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_gender_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read gender data for all subjects\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DBGAP_DIR,\n",
    "                                '3a_dbGaP_SubjectPhenotypes_GenderDS.txt')\n",
    "    df = pd.read_csv(filepath,\n",
    "                     delimiter='\\t',\n",
    "                     dtype={'SUBJID': str})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_demographic_data(filepaths=None):\n",
    "    \"\"\"\n",
    "    Read demographic data for all subjects (child, mother, father)\n",
    "    \"\"\"\n",
    "    if not filepaths:\n",
    "        filenames = ['3a_dbGaP_SubjectPhenotypes_DemographicsDS.txt',\n",
    "                     '3a_dbGaP_SubjectPhenotypes_MaternalDemographicsDS-fixed-03-09-2018.txt',\n",
    "                     '3a_dbGaP_SubjectPhenotypes_PaternalDemographicsDS-fixed-03-09-2018.txt']\n",
    "\n",
    "        filepaths = [os.path.join(DBGAP_DIR, filename)\n",
    "                     for filename in filenames\n",
    "                     ]\n",
    "\n",
    "    child_demo_df = pd.read_csv(os.path.join(filepaths[0]),\n",
    "                                delimiter='\\t',\n",
    "                                dtype={'SUBJID': str})\n",
    "\n",
    "    mother_demo_df = pd.read_csv(os.path.join(filepaths[1]),\n",
    "                                 delimiter='\\t',\n",
    "                                 dtype={'SUBJID': str})\n",
    "\n",
    "    father_demo_df = pd.read_csv(os.path.join(filepaths[2]),\n",
    "                                 delimiter='\\t',\n",
    "                                 dtype={'SUBJID': str})\n",
    "\n",
    "    # Combine demographics of all subjects\n",
    "    subject_demo_df = pd.concat(\n",
    "        [child_demo_df, mother_demo_df, father_demo_df])\n",
    "\n",
    "    subject_demo_df.drop_duplicates('SUBJID', inplace=True)\n",
    "\n",
    "    # Subset of columns\n",
    "    subject_demo_df = subject_demo_df[['RACE', 'ETHNICITY', 'SUBJID']]\n",
    "\n",
    "    def func(row): return \"_\".join(['demographic', str(row.name)])\n",
    "    subject_demo_df['demographic_id'] = subject_demo_df.apply(func, axis=1)\n",
    "\n",
    "    return subject_demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_demographics():\n",
    "    family_df = read_family_data()\n",
    "\n",
    "    # Maternal demographics\n",
    "    filepath = os.path.join(DBGAP_DIR, '3a_dbGaP_SubjectPhenotypes_MaternalDemographicsDS.txt')\n",
    "    mother_demo_df = pd.read_csv(filepath,\n",
    "                                     delimiter='\\t',\n",
    "                                     dtype={'SUBJID': str})\n",
    "    mother_demo_df = pd.merge(mother_demo_df, family_df, left_on='SUBJID', right_on='subjid')\n",
    "    mother_demo_df.drop(columns=['SUBJID', 'subjid', 'father', 'famid', 'is_proband'], inplace=True)\n",
    "    mother_demo_df.rename(columns={'mother': 'SUBJID'}, inplace=True)\n",
    "    mother_demo_df['SUBJID'] = mother_demo_df['SUBJID'].astype('str') \n",
    "    mother_demo_df.to_csv(filepath.split('.')[0] + '-fixed-03-09-2018.txt', sep='\\t', index=False)\n",
    "\n",
    "    # Paternal demographics\n",
    "    filepath = os.path.join(DBGAP_DIR, '3a_dbGaP_SubjectPhenotypes_PaternalDemographicsDS.txt')\n",
    "    father_demo_df = pd.read_csv(filepath,\n",
    "                                     delimiter='\\t',\n",
    "                                     dtype={'SUBJID': str})\n",
    "    father_demo_df = pd.merge(father_demo_df, family_df, left_on='SUBJID', right_on='subjid')\n",
    "    father_demo_df.drop(columns=['SUBJID', 'subjid', 'mother', 'famid', 'is_proband'], inplace=True)\n",
    "    father_demo_df.rename(columns={'father': 'SUBJID'}, inplace=True)\n",
    "    father_demo_df['SUBJID'] = father_demo_df['SUBJID'].astype('str') \n",
    "    father_demo_df.to_csv(filepath.split('.')[0] + '-fixed-03-09-2018.txt', sep='\\t', index=False)\n",
    "fix_demographics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = read_demographic_data()\n",
    "demo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_diagnosis_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read diagnoses data for all subjects\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filename = '3a_dbGaP_SubjectPhenotypes_PatientDiagnosisDS.txt'\n",
    "        filepath = os.path.join(DBGAP_DIR, filename)\n",
    "\n",
    "    diagnosis_df = pd.read_csv(filepath,\n",
    "                               delimiter='\\t',\n",
    "                               dtype={'SUBJID': str})\n",
    "\n",
    "    def func(row): return \"_\".join(['diagnosis', str(row.name)])\n",
    "    diagnosis_df['diagnosis_id'] = diagnosis_df.apply(func, axis=1)\n",
    "\n",
    "    return diagnosis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_subject_sample_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Read sample metadata for all subjects\n",
    "    \"\"\"\n",
    "    if not filepath:\n",
    "        filename = '6a_dbGaP_SubjectSampleMappingDS.txt'\n",
    "        filepath = os.path.join(DBGAP_DIR, filename)\n",
    "\n",
    "    subject_sample_df = pd.read_csv(filepath,\n",
    "                                    delimiter='\\t',\n",
    "                                    dtype={'SUBJID': str})\n",
    "    subject_sample_df.drop_duplicates('SUBJID', inplace=True)\n",
    "\n",
    "    return subject_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliquot\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_sample_shipping_manifest_data(*filepaths):\n",
    "    \"\"\"\n",
    "    Read shipping manifest for samples (from PI/sample source center)\n",
    "    \"\"\"\n",
    "    if not filepaths:\n",
    "        filepaths = [os.path.join(ALIQUOT_SHIP_DIR, filename)\n",
    "\n",
    "                     for filename in os.listdir(ALIQUOT_SHIP_DIR)\n",
    "                     ]\n",
    "\n",
    "    # Combine all manifest files\n",
    "    dfs = [pd.read_excel(filepath,\n",
    "                         delimiter='/t',\n",
    "                         dtype={'*barcode': str},\n",
    "                         skiprows=[0, 1],\n",
    "                         header=[6])\n",
    "\n",
    "           for filepath in filepaths\n",
    "\n",
    "           if os.path.basename(filepath).startswith(\"PCGC\")\n",
    "\n",
    "           ]\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = map((lambda x: x.lower().lstrip(\"*\")), df.columns)\n",
    "    \n",
    "    # Subset of columns\n",
    "    df = df[['barcode',\n",
    "             'external_id',\n",
    "             'sample_collection_site',\n",
    "             'sample_role',\n",
    "             'concentration_ng_per_ul',\n",
    "             'initial_volume_microliters']]\n",
    "\n",
    "    # Drop rows where id cols are nan\n",
    "    id_cols = [col for col in df.columns if \"id\" in col]\n",
    "    df.dropna(subset=id_cols, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing experiment (from read group metadata)\n",
    "@reformat_column_names\n",
    "@dropna_rows_cols\n",
    "def read_seq_experiment_data(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR, \"seidman_metadata.xlsx\")\n",
    "\n",
    "    df = pd.read_excel(filepath, dtype={\"date\": str})\n",
    "    # Rename some columns\n",
    "    df.rename(columns={\"library_name (in original BAM header)\":\n",
    "                       \"library_name\",\n",
    "                       \"barcode\": \"rg_barcode\"}, inplace=True)\n",
    "    df[\"read_length\"] = df[\"read_length\"].apply(\n",
    "        lambda x: int(x.split(\"x\")[0]))\n",
    "    \n",
    "    # Create new columns\n",
    "    df['max_insert_size'] = df['insert_size'].max()\n",
    "    df['mean_insert_size'] = df['insert_size'].mean()\n",
    "    df['mean_read_length'] = df['read_length'].mean()\n",
    "    df['total_reads'] = df['read_length'].count()\n",
    "    \n",
    "    # Subset of columns\n",
    "    df = df[['sample_name',\n",
    "             'library_name',\n",
    "             'rg_barcode',\n",
    "             'run_name',\n",
    "             'read_length',\n",
    "             'date',\n",
    "             'library_strategy',\n",
    "             'library_source',\n",
    "             'library_selection',\n",
    "             'insert_size',\n",
    "             'instrument',\n",
    "             'library_layout',\n",
    "             'max_insert_size',\n",
    "             'mean_insert_size',\n",
    "             'mean_read_length',\n",
    "             'total_reads']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction - Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_files_df = read_study_file_data()\n",
    "study_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family \n",
    "family_df = read_family_data()\n",
    "family_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenotypes\n",
    "phenotype_df = read_phenotype_data()\n",
    "phenotype_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender\n",
    "gender_df = read_gender_data()\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic\n",
    "demographic_df = read_demographic_data()\n",
    "demographic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis data\n",
    "diagnosis_df = read_diagnosis_data()\n",
    "diagnosis_df.head()\n",
    "# diagnosis_df[diagnosis_df['subjid'] == '279']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "subject_sample_df = read_subject_sample_data()\n",
    "subject_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliquot/Sample Shipping data\n",
    "shipping_manifest_df = read_sample_shipping_manifest_data()\n",
    "shipping_manifest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing experiments\n",
    "seq_exp_df = read_seq_experiment_data()\n",
    "seq_exp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants\n",
    "print(\"Family\")\n",
    "print(family_df.nunique())\n",
    "print(\"\\nDemographics\")\n",
    "print(demographic_df.nunique())\n",
    "print(\"\\nGender\")\n",
    "print(demographic_df.nunique())\n",
    "print(\"\\nDiagnosis\")\n",
    "print(demographic_df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigator\n",
    "investigator_df = read_investigator_data()\n",
    "investigator_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study\n",
    "study_df = read_study_data()\n",
    "study_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study files\n",
    "study_files_df = read_study_file_data()\n",
    "study_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family\n",
    "family_df = read_family_data()\n",
    "family_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create participant df\n",
    "# Merge Gender + Demographics\n",
    "gender_demo_df = pd.merge(gender_df, demographic_df, on='subjid')\n",
    "# Add Family\n",
    "df1 = pd.merge(gender_demo_df, family_df, on='subjid')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Diagnosis\n",
    "df2 = pd.merge(df1, diagnosis_df, on='subjid')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge Sample\n",
    "df3 = pd.merge(df2, subject_sample_df, on='subjid')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge Aliquot\n",
    "df4 = pd.merge(df3, shipping_manifest_df, left_on='sampid', right_on='external_id')\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Sequencing Experiment\n",
    "full_participant_df = pd.merge(df4, seq_exp_df, left_on='external_id', right_on='sample_name')\n",
    "full_participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study\n",
    "study = {\n",
    "    'data_access_authority': 'dbGaP',\n",
    "    'study_id': 'phs001138',\n",
    "    'study_version': 'v1.p2',\n",
    "    'study_name': 'Discovery of the Genetic Basis of Structural Heart'\n",
    "    'and Other Birth Defects',\n",
    "    'attribution': 'https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/'\n",
    "    'GetAcknowledgementStatement.cgi?study_id=phs001138.v1.p2'\n",
    "}\n",
    "study_df = pd.DataFrame([study])\n",
    "study_df.to_csv(os.path.join(DATA_DIR, 'study.txt'))\n",
    "\n",
    "# Create investigator\n",
    "invest = {\n",
    "    'investigator_name': 'Christine E. Seidman',\n",
    "    'institution': 'Harvard Medical School'\n",
    "}\n",
    "inv_df = pd.DataFrame([invest])\n",
    "inv_df.to_csv(os.path.join(DATA_DIR, 'investigator.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_study_cols(study_df, df):\n",
    "    # Add study cols to a df\n",
    "    cols = study_df.columns.tolist()\n",
    "    row = study_df.iloc[0]\n",
    "    for col in cols:\n",
    "        df[col] = row[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add study to full participant df\n",
    "_add_study_cols(study_df, full_participant_df)\n",
    "\n",
    "# Add study to basic participant df\n",
    "participant_df = _add_study_cols(study_df, family_df)\n",
    "\n",
    "# Add study to investigator df\n",
    "study_investigator_df =_add_study_cols(study_df, investigator_df)\n",
    "\n",
    "# Add study to study files df\n",
    "study_study_files_df = _add_study_cols(study_df, study_files_df)\n",
    "\n",
    "# Phenotype df\n",
    "phenotype_participant_df = pd.merge(phenotype_df, participant_df,\n",
    "                                    on='subjid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "n = 22\n",
    "chunk_size = 10\n",
    "entity_type ='participant'\n",
    "entities = ['{}_{}'.format(entity_type, j) for j in range(n)]\n",
    "for i in range(0, n, chunk_size):\n",
    "    chunk = entities[i - chunk_size:i]\n",
    "    if chunk:\n",
    "        start = i - chunk_size + 1\n",
    "        print('Adding {}:{} {}s to session'.format(start, i,\n",
    "                                                   entity_type))\n",
    "        pprint([e for e in entities[start:i]])\n",
    "        print('Flushing {} {}\\ns'.format(chunk_size, entity_type))\n",
    "\n",
    "print('Flushing remaining {} {}s to session\\n'.format(\n",
    "    len(entities[i:]) + 1, entity_type))\n",
    "\n",
    "remaining = entities[0:1] + entities[i:]\n",
    "\n",
    "pprint([e for e in remaining])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(filepath):\n",
    "    with open(filepath, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "def write_json(data, filepath):\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        json.dump(data, json_file, sort_keys=True, indent=4, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(DATA_DIR, 'genomic_file_uuid.json')\n",
    "file_json = read_json(fp)\n",
    "write_json(file_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genomic file info df\n",
    "def read_genomic_file_info(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR, 'genomic_file_uuid.json')\n",
    "        \n",
    "    def get_ext(fp):\n",
    "        filename = os.path.basename(fp)\n",
    "        parts = filename.split('.')\n",
    "        if len(parts) > 2:\n",
    "            ext = '.'.join(parts[1:])\n",
    "        else:\n",
    "            ext = parts[-1]\n",
    "        return ext\n",
    "\n",
    "    with open(filepath, 'r') as json_file:\n",
    "        uuid_dict = json.load(json_file)\n",
    "\n",
    "    gf_dicts = []\n",
    "    for k, v in uuid_dict.items():\n",
    "        file_info = {\n",
    "            'uuid': v['did'],\n",
    "            'md5sum': v['hashes']['md5'],\n",
    "            'file_url': v['urls'][0],\n",
    "            'file_size': v['size'],\n",
    "            'data_type': 'submitted aligned reads',\n",
    "            'file_format': get_ext(v['urls'][0]),\n",
    "            'file_name': os.path.basename(v['urls'][0])\n",
    "        }\n",
    "        gf_dicts.append(file_info)\n",
    "        \n",
    "    return pd.DataFrame(gf_dicts)\n",
    "\n",
    "def read_sample_gf_data(filepath=None):\n",
    "    if not filepath:\n",
    "        filepath = os.path.join(DATA_DIR, 'manifests', 'GMKF_BAMsampleIDs.xlsx')\n",
    "    df = pd.read_excel(filepath)\n",
    "    df = df.loc[df['Cohort'] == 'GMKF-Seidman']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read genomic file info\n",
    "gf_file_info_df = read_genomic_file_info()\n",
    "# Sample and BAM File df\n",
    "sample_gf_df = read_sample_gf_data()\n",
    "# Merge with sequencing experiment df\n",
    "df1 = pd.merge(sample_gf_df, seq_exp_df, left_on='dbgap_subject_id', right_on='sample_name')\n",
    "# Merge with genomic file info df\n",
    "df2 = pd.merge(df1, gf_file_info_df, left_on='BAM sample ID', right_on='file_name')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe(include=['O']).T.sort_values('unique', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_genomic_file_info()\n",
    "print(df['file_size'].max()/1000000000)\n",
    "print(df['file_size'].min()/1000000000)\n",
    "print(df['file_size'].mean()/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_phenotype_data()\n",
    "df['observed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/singhn4/Desktop/phenotype.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where((pd.notnull(df)), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
